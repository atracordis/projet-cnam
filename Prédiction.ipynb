{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaoEu82dVAmUFlkGixVtSR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Attention\n","\n","NB: Puisque le projet se fait sur Google Colab, nous devons utiliser Drive pour stocker les fichiers de façon durable.\n","\n","Si vous exécuter le code localement, vous n'avez pas besoin de ce bloc de code. Il faudra penser à mettre à jour la variable root_path deux blocs plus loin."],"metadata":{"id":"JEXnausCXSOZ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2VEdfDJQBXP","executionInfo":{"status":"ok","timestamp":1734601201615,"user_tz":-60,"elapsed":1630,"user":{"displayName":"Wajd Meskini","userId":"05225014304211613605"}},"outputId":"301303ba-e1ca-46b3-acaf-b08beb9323d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# On s'assure que vous avez exactement les mêmes versions que les librairies utilisées dans ce notebook\n","!pip install -q pandas==2.2.2 scikit-learn==1.5.2 matplotlib==3.8.0 numpy==1.26.4 tqdm==4.66.6 imbalanced-learn==0.12.4 shap==0.46.0 equipy==0.0.6a0.dev0"],"metadata":{"id":"V37WjE32QBqa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prédictions\n","\n","Le but de ce notebook est d'illustrer la partie prédiction. Nous avons déjà entraîné un modèle dans le notebook Modélisation, et pour plus de visibilité et vous épargner sa lecture, nous avons exporté le meilleur modèle avec la meilleure approche, pour l'appliquer ici sur le jeu de test et l'exporter comme convenu."],"metadata":{"id":"qY-WkJ1lUHIj"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","random_state=24\n","\n","root_path = \"/content/drive/My Drive/Projet CNAM\"\n","path_test = f\"{root_path}/test.csv\"\n","\n","df_test = pd.read_csv(path_test, sep=\",\")\n","df_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66_wk6jEQC6I","executionInfo":{"status":"ok","timestamp":1734601213807,"user_tz":-60,"elapsed":2678,"user":{"displayName":"Wajd Meskini","userId":"05225014304211613605"}},"outputId":"7cd868ad-2b29-4cae-aa35-f242b32b465a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(30000, 11)"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["Le code nécessite de réutiliser le même code de preprocessing défini dans la partie Modélisation et réuni dans une pipeline."],"metadata":{"id":"gsuxVSzMUXVk"}},{"cell_type":"code","source":["import numpy as np\n","\n","def preprocess_quali(df):\n","    feats_quali=[]\n","    # on s'occupe d'abord des variables binaires\n","    # preference : valeur = 1 <=> corrélation positive avec cible\n","    df[\"feat_permis\"]=(df[\"permisConduire\"]==1).astype(int)\n","    feats_quali.append(\"feat_permis\")\n","    df[\"feat_sans_assurance\"]=(df[\"dejaAssure\"]==0).astype(int)\n","    feats_quali.append(\"feat_sans_assurance\")\n","    df[\"feat_accident\"]=(df[\"accidentVehicule\"]==\"Yes\").astype(int)\n","    feats_quali.append(\"feat_accident\")\n","    df[\"feat_homme\"]=(df[\"genre\"]==\"Male\").astype(int)\n","    feats_quali.append(\"feat_homme\")\n","    # on traite ensuite la valeur multicatégorielle (ageVehicle)\n","    df[\"feat_age_vehicule>1\"]=(df.ageVehicule.isin([\"1-2 Year\", \"> 2 Years\"])).astype(int)\n","    feats_quali.append(\"feat_age_vehicule>1\")\n","    return df, feats_quali\n","\n","def truncate_quantile(series, quantile=0.95):\n","    q95=int(series.quantile(quantile))\n","    series_output=series.copy()\n","    series_output[series_output>q95]=q95+1\n","    return series_output\n","\n","def preprocess_quanti(df, norm_dict={}):\n","    feats_quanti=[]\n","    # on corrige d'abord la prime annuelle\n","    two_trailing_zeroes=(df[\"primeAnnuelle\"].astype(str).str[-2:]==\"00\")\n","    big_premium=(df[\"primeAnnuelle\"]>50000)\n","    df[\"feat_prime\"]=df[\"primeAnnuelle\"]\n","    df.loc[big_premium & two_trailing_zeroes,\"feat_prime\"]=df.loc[big_premium & two_trailing_zeroes,\"primeAnnuelle\"]/100\n","    feats_quanti.append(\"feat_prime\")\n","    # on corrige ensuite les âges\n","    df[\"feat_age\"]=truncate_quantile(df[\"age\"])\n","    df[\"feat_age_q95\"]=(df[\"feat_age\"]==df[\"feat_age\"].max()).astype(int)\n","    feats_quanti.append(\"feat_age\")\n","    # on corrige enfin le temps assuré\n","    df[\"feat_temps_assure\"]=truncate_quantile(df[\"tempsAssure\"])\n","    df[\"feat_temps_assure_q95\"]=(df[\"feat_temps_assure\"]==df[\"feat_temps_assure\"].max()).astype(int)\n","    feats_quanti.append(\"feat_temps_assure\")\n","    # on normalise: si on n'a pas de dictionnaire pour normaliser, on en crée un et on l'utilise\n","    if not norm_dict:\n","        for col in feats_quanti:\n","            norm_dict[col]={\"min\":df[col].min(), \"max\":df[col].max()}\n","    for col in feats_quanti:\n","        df[col]=(df[col]-norm_dict[col][\"min\"])/(norm_dict[col][\"max\"]-norm_dict[col][\"min\"])\n","        # pour garder la valeur dans le périmètre dans le cas du jeu de test et d'eval\n","        df.loc[df[col]>1,col]=1\n","        df.loc[df[col]<0,col]=0\n","    feats_quanti.extend([\"feat_age_q95\", \"feat_temps_assure_q95\"])\n","    return df, norm_dict, feats_quanti\n","\n","def process_other(df, dict_cat={}):\n","    no_dict_cat=False\n","    feats_other=[]\n","    target_prop=0\n","    if not dict_cat:\n","        no_dict_cat=True\n","        target_prop=df[target].mean()\n","    list_other=['codeRegion', 'canalDistribution']\n","    for col in list_other:\n","        if no_dict_cat:\n","            dict_cat[col]={}\n","            # identification des classes majoritaires\n","            df_proportions=pd.DataFrame(df[col].value_counts(normalize=True)).reset_index(drop=False)\n","            # on identifie le \"coude\" comme étant l'instant du plus grand écart absolu d'une catégorie à la suivante\n","            df_proportions[\"diff\"]=df_proportions.proportion.diff().shift(-1).abs()\n","            max_prop=df_proportions.loc[df_proportions[\"diff\"]==df_proportions[\"diff\"].max(),\"proportion\"].iloc[0]\n","            # les classes majoritaires sont celles à gauche du coude\n","            dict_cat[col][\"list_maj\"]=df_proportions.loc[df_proportions.proportion>=max_prop, col].tolist()\n","            # identification des classes minoritaires positives\n","            df_other=df[~df[col].isin(dict_cat[col][\"list_maj\"])]\n","            df_crosstab=pd.crosstab(df_other[target], df_other[col], normalize=\"columns\").transpose().reset_index()\n","            # une classe est minoritaire positive si sa proportion de valeurs positives > proportion moyenne du dataset\n","            dict_cat[col][\"list_min_pos\"]=df_crosstab.loc[df_crosstab[1]>=target_prop, col].tolist()\n","            dict_cat[col][\"list_min_neg\"]=df_crosstab.loc[df_crosstab[1]<target_prop, col].tolist() # respectivement, minoritaire négative\n","        #application : on initialise la colonne à l'identique pour maintenir les classes majoritaires\n","        df[\"feat_\"+col]=df[col].astype(str)\n","        # si une classe appartient aux classes minoritaires (positives ou négatives), elle reçoit le label correspondant\n","        df.loc[df[col].isin(dict_cat[col][\"list_min_pos\"]), \"feat_\"+col]=f\"pos\"\n","        df.loc[df[col].isin(dict_cat[col][\"list_min_neg\"]), \"feat_\"+col]=f\"neg\"\n","        # si une classe n'est dans aucune catégorie, elle reçoit le label \"autre\"\n","        df.loc[~df[col].isin(dict_cat[col][\"list_maj\"]+dict_cat[col][\"list_min_pos\"]+dict_cat[col][\"list_min_neg\"]), \"feat_\"+col]=f\"other\"\n","        # on applique un one hot encoding personnalisé pour tenir compte du fait qu'on ne veut pas de \"other\"\n","        # on drop également les colonnes minoritaires négatives + other pour éviter les multicolinéarités\n","        for i in [i for i in df[f\"feat_{col}\"].unique() if i not in [f\"other_{col}\", f\"neg_{col}\"]]:\n","            df[f\"feat_{col}_{i}\"]=(df[f\"feat_{col}\"]==i).astype(int)\n","            feats_other.append(f\"feat_{col}_{i}\")\n","\n","    return df, dict_cat, feats_other"],"metadata":{"id":"FjGtDpivQM_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import sklearn\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline\n","\n","class Preprocessor(BaseEstimator, TransformerMixin):\n","# la classe doit avoir des fonctions que sklearn connaît\n","# telles que fit, transform et fit_transform\n","# sklearn la traitera donc comme un modèle comme les autres\n","  def __init__(self, list_feat):\n","    self.norm_dict = None\n","    self.dict_cat = None\n","    self.list_feat = list_feat\n","    return None\n","\n","  def fit(self, X, y=None):\n","    self.fit_transform(X)\n","    return self\n","\n","  def fit_transform(self, X, y=0):\n","    # fit_transform gère le jeu de train et initialise les valeurs\n","    X, _=preprocess_quali(X.copy())\n","    X, self.norm_dict, _=preprocess_quanti(X)\n","    X, self.dict_cat, _=process_other(X)\n","    return X[self.list_feat]\n","\n","  def transform(self, X):\n","    # transform sert à traiter le jeu de test avec les valeurs déjà initialisées\n","    # grâce au jeu de train\n","    X, _=preprocess_quali(X.copy())\n","    if self.norm_dict:\n","        X, _, _=preprocess_quanti(X, self.norm_dict)\n","    else:\n","        print(\"no norm_dict initialized\")\n","        raise ValueError\n","    if self.dict_cat:\n","        X, _, _=process_other(X, self.dict_cat)\n","    else:\n","        print(\"no dict_cat initialized\")\n","        raise ValueError\n","    return X[self.list_feat]"],"metadata":{"id":"efiJyx05QzJY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nous allons donc lire le pickle exporté dans l'autre notebook, et l'appliquer sur le jeu de test pour avoir les prédictions et les probabilités\n","\n","Le modèle inclut déjà le preprocessing et l'application d'un seuil personnalisé"],"metadata":{"id":"zICoG9S-UhWr"}},{"cell_type":"code","source":["import pickle\n","filename = f\"{root_path}/best_model.pkl\"\n","\n","model_card = pickle.load(open(filename, 'rb'))\n","\n","list_feat=model_card[\"list_feat\"]\n","target=model_card[\"target\"]\n","model=model_card[\"model\"]"],"metadata":{"id":"10tArRnNSe6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test[\"interesse\"]=model.predict(df_test)\n","df_test[\"probability\"]=model.predict_proba(df_test)[:, 1]"],"metadata":{"id":"H3k3GBmfSuy0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path_output = f\"{root_path}/prediction_11.csv\"\n","\n","df_test[[\"id\", \"probability\", \"interesse\"]].to_csv(path_output, sep=\",\")"],"metadata":{"id":"o-yk9M4ITgxD","executionInfo":{"status":"ok","timestamp":1734601883288,"user_tz":-60,"elapsed":10,"user":{"displayName":"Wajd Meskini","userId":"05225014304211613605"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7vSv3VVXXBBg"},"execution_count":null,"outputs":[]}]}